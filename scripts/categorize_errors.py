#!/usr/bin/env python3
"""
Categorize NLI model errors into different types for analysis.
Reads the error TSV generated by analyze_errors.py and categorizes them.
Updated to focus on categories that contrastive learning should improve.
"""

import csv
import random
from collections import defaultdict
import argparse
import datasets
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

# Download required NLTK data (will only download if not present)
for resource in ['punkt', 'punkt_tab', 'stopwords', 'wordnet', 'averaged_perceptron_tagger_eng']:
    try:
        if 'punkt' in resource:
            nltk.data.find(f'tokenizers/{resource}')
        elif resource in ['stopwords', 'wordnet']:
            nltk.data.find(f'corpora/{resource}')
        elif 'tagger' in resource:
            nltk.data.find(f'taggers/{resource}')
    except LookupError:
        nltk.download(resource.replace('_eng', ''), quiet=True)


def calculate_lexical_overlap(text1, text2):
    """Calculate Jaccard similarity (lexical overlap) between two texts using NLTK."""
    # Initialize stemmer and get stopwords
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words('english'))

    # Tokenize using NLTK
    tokens1 = word_tokenize(text1.lower())
    tokens2 = word_tokenize(text2.lower())

    # Filter out stopwords and non-alphabetic tokens, then stem
    words1 = set()
    for token in tokens1:
        if token.isalpha() and token not in stop_words:
            words1.add(stemmer.stem(token))

    words2 = set()
    for token in tokens2:
        if token.isalpha() and token not in stop_words:
            words2.add(stemmer.stem(token))

    if not words1 or not words2:
        return 0.0

    # Jaccard similarity: intersection / union
    intersection = len(words1 & words2)
    union = len(words1 | words2)

    return intersection / union if union > 0 else 0.0


def find_high_overlap_premises():
    """Find premises from SNLI eval set with high lexical overlap between hypotheses."""
    print("Loading SNLI validation set to find high-overlap premise groups...")

    # Load SNLI validation set
    snli = datasets.load_dataset('snli', split='validation')

    # Group by premise
    premise_groups = defaultdict(list)
    for example in snli:
        if example['label'] != -1:  # Skip examples with no label
            premise_groups[example['premise']].append({
                'hypothesis': example['hypothesis'],
                'label': example['label']
            })

    # Calculate overlaps for each premise group
    premise_overlaps = []

    for premise, hypotheses in premise_groups.items():
        if len(hypotheses) >= 2:
            # Calculate max overlap for this premise
            max_overlap = 0.0
            for i in range(len(hypotheses)):
                for j in range(i + 1, len(hypotheses)):
                    if hypotheses[i]['label'] != hypotheses[j]['label']:
                        overlap = calculate_lexical_overlap(
                            hypotheses[i]['hypothesis'],
                            hypotheses[j]['hypothesis']
                        )
                        max_overlap = max(max_overlap, overlap)

            if max_overlap > 0:
                premise_overlaps.append((premise, max_overlap))

    # Sort by overlap and get top 50%
    target_pct = 0.2
    premise_overlaps.sort(key=lambda x: x[1], reverse=True)
    top_count = max(1, int(len(premise_overlaps) * target_pct))
    bottom_count = max(1, int(len(premise_overlaps) * target_pct))
    high_overlap_premises = set(p[0] for p in premise_overlaps[:top_count])
    low_overlap_premises = set(p[0] for p in premise_overlaps[-bottom_count:])
    top_3_overlaps = [p[0] for p in premise_overlaps[:3]]
    bottom_3_overlaps = [p[0] for p in premise_overlaps[-3:]]

    # print count high and low
    # print(f"Total premises analyzed for overlap: {len(premise_groups)}")
    # print(f"Total premises with multiple hypotheses: {len(premise_overlaps)}")
    # print(f"High-overlap premises (top {target_pct*100:.0f}%): {len(high_overlap_premises)}")
    # print(f"Low-overlap premises (bottom {100 - target_pct*100:.0f}%): {len(low_overlap_premises)}")
    #
    # if premise_overlaps:
    #     threshold = premise_overlaps[min(top_count, len(premise_overlaps)-1)][1]
    #     print(f"Found {len(high_overlap_premises)} high-overlap premises (top {target_pct*100:.0f}%)")
    #     print(f"Overlap threshold for top {target_pct*100:.0f}%: {threshold:.3f}")
    #
    # # print premise/hypotheses for each high-overlap premise
    # print("\nHigh-overlap premises and their hypotheses:")
    # for premise in top_3_overlaps:
    #     print(f'\nPremise: "{premise}"')
    #     for hyp in premise_groups[premise]:
    #         print(f'  Hypothesis: "{hyp["hypothesis"]}" (label: {hyp["label"]})')
    #
    # # print premise/hypotheses for each low-overlap premise
    # print("\nLow-overlap premises and their hypotheses:")
    # for premise in bottom_3_overlaps:
    #     print(f'\nPremise: "{premise}"')
    #     for hyp in premise_groups[premise]:
    #         print(f'  Hypothesis: "{hyp["hypothesis"]}" (label: {hyp["label"]})')

    return high_overlap_premises, low_overlap_premises


def get_word_tokens(text):
    """Get word tokens from text, lowercased."""
    return [t.lower() for t in word_tokenize(text) if t.isalpha()]


def has_negation_flip(premise, hypothesis):
    """Check if negation is added/removed between premise and hypothesis."""
    NEG_WORDS = {"not", "never", "no", "none", "n't", "nobody", "nothing", "neither", "nor"}
    
    premise_tokens = set(get_word_tokens(premise))
    hypothesis_tokens = set(get_word_tokens(hypothesis))
    
    premise_negs = premise_tokens & NEG_WORDS
    hypothesis_negs = hypothesis_tokens & NEG_WORDS
    
    # Check if negation differs between premise and hypothesis
    return len(premise_negs) != len(hypothesis_negs)


def has_quantifier_change(premise, hypothesis):
    """Check if quantifiers change between premise and hypothesis."""
    UNIVERSAL = {"all", "every", "each", "always", "everyone", "everything"}
    EXISTENTIAL = {"some", "any", "someone", "something", "sometimes"}
    NEGATIVE = {"no", "none", "nobody", "nothing", "never"}
    QUANTITY = {"many", "few", "several", "most", "much", "little"}
    
    ALL_QUANTIFIERS = UNIVERSAL | EXISTENTIAL | NEGATIVE | QUANTITY
    
    premise_tokens = set(get_word_tokens(premise))
    hypothesis_tokens = set(get_word_tokens(hypothesis))
    
    premise_quants = premise_tokens & ALL_QUANTIFIERS
    hypothesis_quants = hypothesis_tokens & ALL_QUANTIFIERS
    
    # Changed quantifiers or added/removed them
    return premise_quants != hypothesis_quants


def contains_antonyms(premise, hypothesis):
    """Check if hypothesis contains antonym of a word in premise."""
    premise_tokens = get_word_tokens(premise)
    hypothesis_tokens = get_word_tokens(hypothesis)
    
    for p_word in premise_tokens:
        p_synsets = wordnet.synsets(p_word)
        for p_syn in p_synsets:
            for lemma in p_syn.lemmas():
                if lemma.antonyms():
                    for ant in lemma.antonyms():
                        if ant.name() in hypothesis_tokens:
                            return True
    return False


def has_hypernym_relation(premise, hypothesis):
    """Check if there's a hypernym/hyponym relationship between premise and hypothesis words."""
    premise_tokens = get_word_tokens(premise)
    hypothesis_tokens = get_word_tokens(hypothesis)
    
    for p_word in premise_tokens:
        p_synsets = wordnet.synsets(p_word, pos=wordnet.NOUN)
        for h_word in hypothesis_tokens:
            h_synsets = wordnet.synsets(h_word, pos=wordnet.NOUN)
            
            # Check if h_word is hypernym of p_word
            for p_syn in p_synsets:
                for h_syn in h_synsets:
                    if h_syn in p_syn.hypernyms():
                        return True
                    if p_syn in h_syn.hypernyms():
                        return True
    return False


def has_modifier_addition(premise, hypothesis):
    """Check if hypothesis adds adjectives/adverbs compared to premise."""
    # POS tag both
    premise_pos = nltk.pos_tag(word_tokenize(premise))
    hypothesis_pos = nltk.pos_tag(word_tokenize(hypothesis))
    
    # Extract adjectives (JJ*) and adverbs (RB*)
    premise_modifiers = set([word.lower() for word, pos in premise_pos 
                            if pos.startswith('JJ') or pos.startswith('RB')])
    hypothesis_modifiers = set([word.lower() for word, pos in hypothesis_pos 
                               if pos.startswith('JJ') or pos.startswith('RB')])
    
    # Check if hypothesis has more modifiers
    return len(hypothesis_modifiers - premise_modifiers) > 0


def categorize_errors(tsv_file, seed=42):
    """Categorize errors based on linguistic phenomena that contrastive learning should improve."""
    random.seed(seed)  # For reproducibility

    # Get high-overlap premises from SNLI
    high_overlap_premises, low_overlap_premises = find_high_overlap_premises()
    # lowercase all
    high_overlap_premises = set(p.lower() for p in high_overlap_premises)
    low_overlap_premises = set(p.lower() for p in low_overlap_premises)

    # Read the TSV and analyze error patterns
    errors_by_pattern = defaultdict(list)

    with open(tsv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            # Categorize errors
            premise = row['premise'].lower()
            hypothesis = row['hypothesis'].lower()
            premise_orig = row['premise']
            hypothesis_orig = row['hypothesis']
            true_label = row['true_label']
            pred_label = row['predicted_label']
            confidence = float(row['confidence'])

            categorized = False
            def record_error(category, row):
                nonlocal categorized
                errors_by_pattern[category].append(row)
                categorized = True

            errors_by_pattern['error'].append(row)

            # Calculate lexical overlap for reuse
            p_h_overlap = calculate_lexical_overlap(premise_orig, hypothesis_orig)

            # === PRIMARY CONTRASTIVE LEARNING TARGETS ===
            
            # Only analyze actual errors
            if true_label != pred_label:
                # === CATEGORY A: LEXICAL CONTRAST (Should improve most) ===
                
                # A1: Negation sensitivity
                if has_negation_flip(premise_orig, hypothesis_orig):
                    record_error('negation_flip', row)
                
                # A2: Quantifier contrasts
                if has_quantifier_change(premise_orig, hypothesis_orig):
                    record_error('quantifier_change', row)
                
                # A3: Antonym substitution
                try:
                    if contains_antonyms(premise_orig, hypothesis_orig):
                        record_error('antonym_substitution', row)
                except:
                    pass  # WordNet lookup can fail for some words
                
                # === CATEGORY B: SEMANTIC GRANULARITY (Medium improvement) ===
                
                # B1: Hypernym/hyponym
                try:
                    if has_hypernym_relation(premise_orig, hypothesis_orig):
                        record_error('hypernym_hyponym', row)
                except:
                    pass  # WordNet lookup can fail
                
                # B2: Modifier sensitivity
                if has_modifier_addition(premise_orig, hypothesis_orig):
                    record_error('modifier_addition', row)
                
                # === CONTRASTIVE PREMISE GROUPS ===
                
                # High contrast premise errors (these should definitely improve)
                if premise in high_overlap_premises:
                    record_error('high_contrast_premise', row)
                
                # Low contrast premise errors (control group)
                if premise in low_overlap_premises:
                    record_error('low_contrast_premise', row)
                
                # === OVERLAP-BASED CATEGORIES ===
                
                # High overlap errors (>60% overlap but wrong prediction)
                if p_h_overlap > 0.6:
                    record_error('very_high_overlap', row)
                    if pred_label == 'entailment' and true_label != 'entailment':
                        record_error('high_overlap_false_entailment', row)
                
                # Low overlap errors (<20% overlap)
                elif p_h_overlap < 0.2:
                    record_error('very_low_overlap', row)
                    if pred_label == 'contradiction' and true_label != 'contradiction':
                        record_error('low_overlap_false_contradiction', row)
                
                # === CONFIDENCE-BASED ===
                
                if confidence > 0.95:
                    record_error('high_confidence_error', row)
            
            # === ALWAYS TRACK THESE ===
            
            # Ambiguous (predicting neutral when not expected) - not just errors
            if pred_label == 'neutral':
                record_error('ambiguous', row)

            # Other
            if not categorized: 
                record_error('other', row)

    return errors_by_pattern


def print_error_analysis(errors_by_pattern, num_examples=2):
    """Print categorized error analysis with examples."""

    print('# Error Categories from NLI Model Analysis')
    print('Based on contrastive learning objectives\n')

    categories = [
        # Primary targets of contrastive learning
        ('negation_flip', 'ğŸ”´ Negation Flips',
         'Negation added/removed between premise and hypothesis - PRIMARY TARGET'),
        ('quantifier_change', 'ğŸ”¢ Quantifier Changes',
         'Quantifiers differ between premise and hypothesis - PRIMARY TARGET'),
        ('antonym_substitution', 'â†”ï¸  Antonym Substitutions',
         'Words replaced with antonyms - PRIMARY TARGET'),
        ('hypernym_hyponym', 'ğŸ”º Hypernym/Hyponym Relations',
         'General/specific term substitutions - SHOULD IMPROVE'),
        ('modifier_addition', 'â• Modifier Additions',
         'Adjectives/adverbs added to hypothesis - SHOULD IMPROVE'),
        # Premise-based categories
        ('high_contrast_premise', 'âš¡ High Contrast Premise Errors',
         'Errors on premises with high-overlap different-label hypotheses - KEY TARGET'),
        ('low_contrast_premise', 'ğŸ’¤ Low Contrast Premise Errors',
         'Errors on premises with low-overlap hypotheses - CONTROL GROUP'),
        # Overlap-based
        ('very_high_overlap', 'ğŸ“Š Very High Overlap (>60%)',
         'High lexical overlap between premise and hypothesis'),
        ('high_overlap_false_entailment', 'âŒ High Overlap â†’ False Entailment',
         'Model wrongly assumes high overlap means entailment'),
        ('very_low_overlap', 'ğŸ“‰ Very Low Overlap (<20%)',
         'Low lexical overlap between premise and hypothesis'),
        ('low_overlap_false_contradiction', 'âŒ Low Overlap â†’ False Contradiction',
         'Model wrongly assumes low overlap means contradiction'),
        # Other
        ('high_confidence_error', 'ğŸ”¥ High Confidence Errors (>95%)',
         'Model is very confident but wrong'),
        ('ambiguous', 'Ambiguous Examples',
         'Model predicts neutral'),
        ('other', 'Other Errors', 'Uncategorized errors')
    ]

    for pattern_key, pattern_name, description in categories:
        examples = errors_by_pattern[pattern_key]
        if examples:
            print(f'\n## {len(examples)} errors: {pattern_name}')
            print(f'{description}')
            print('\nExamples:')
            # Show num_examples per category
            for i, ex in enumerate(random.sample(examples, min(num_examples, len(examples))), 1):
                print(f'\n{i}. Premise: "{ex["premise"][:120]}"')
                print(f'   Hypothesis: "{ex["hypothesis"][:120]}"')
                print(f'   Gold: {ex["true_label"]:12} â†’ Predicted: {ex["predicted_label"]:12} (confidence: {ex["confidence"]})')


def save_categorized_errors(errors_by_pattern, output_file):
    """Save categorized errors to a new TSV file with category column."""

    all_categorized = []

    for category, errors in errors_by_pattern.items():
        for error in errors:
            error_copy = dict(error)
            error_copy['error_category'] = category
            all_categorized.append(error_copy)

    if all_categorized:
        # Get fieldnames from first error, add category
        fieldnames = list(all_categorized[0].keys())
        if 'error_category' not in fieldnames:
            fieldnames.append('error_category')

        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\t')
            writer.writeheader()
            writer.writerows(all_categorized)

        print(f'\nSaved {len(all_categorized)} categorized errors to {output_file}')


def main():
    parser = argparse.ArgumentParser(description='Categorize NLI model errors')
    parser.add_argument('--input', type=str, default='all_errors.tsv',
                        help='Input TSV file with errors')
    parser.add_argument('--output', type=str, default=None,
                        help='Output TSV file with categorized errors (optional)')
    parser.add_argument('--num-examples', type=int, default=2,
                        help='Number of examples to show per category')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed for example selection')

    args = parser.parse_args()

    # Categorize errors
    errors_by_pattern = categorize_errors(args.input, args.seed)

    # Print analysis
    # print_error_analysis(errors_by_pattern, args.num_examples)

    # Save if requested
    if args.output:
        save_categorized_errors(errors_by_pattern, args.output)

    # Print summary statistics
    print('\n' + '='*70)
    print('SUMMARY STATISTICS')
    print('='*70)
    total_errors = len(errors_by_pattern.get('error', []))
    print(f'Total errors analyzed: {total_errors}')
    
    # Categories that contrastive learning should improve most
    primary_targets = ['negation_flip', 'quantifier_change', 'antonym_substitution']
    secondary_targets = ['hypernym_hyponym', 'modifier_addition']
    premise_based = ['high_contrast_premise', 'low_contrast_premise']
    
    print('\nPRIMARY TARGETS (should improve significantly with contrastive learning):')
    for cat in primary_targets:
        if cat in errors_by_pattern:
            count = len(errors_by_pattern[cat])
            pct = (count / total_errors * 100) if total_errors > 0 else 0
            print(f'  {cat:25} {count:5} ({pct:5.1f}%)')
    
    print('\nSECONDARY TARGETS (should improve moderately):')
    for cat in secondary_targets:
        if cat in errors_by_pattern:
            count = len(errors_by_pattern[cat])
            pct = (count / total_errors * 100) if total_errors > 0 else 0
            print(f'  {cat:25} {count:5} ({pct:5.1f}%)')
    
    print('\nCONTRASTIVE PREMISE GROUPS:')
    for cat in premise_based:
        if cat in errors_by_pattern:
            count = len(errors_by_pattern[cat])
            pct = (count / total_errors * 100) if total_errors > 0 else 0
            print(f'  {cat:25} {count:5} ({pct:5.1f}%)')
    
    # Remove 'error' before showing all categories
    if 'error' in errors_by_pattern:
        del errors_by_pattern['error']
    
    print('\nALL CATEGORIES (sorted by frequency):')
    for category, errors in sorted(errors_by_pattern.items(), key=lambda x: len(x[1]), reverse=True):
        percentage = len(errors) / total_errors * 100 if total_errors > 0 else 0
        print(f'  {category:30} {len(errors):5} ({percentage:5.1f}%)')


if __name__ == "__main__":
    main()
